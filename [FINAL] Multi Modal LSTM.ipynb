{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eba46c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v2 as tf\n",
    "\n",
    "import collections\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from keras import activations\n",
    "from keras import backend\n",
    "from keras import constraints\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras.engine.base_layer import Layer\n",
    "from keras.engine.input_spec import InputSpec\n",
    "from keras.saving.saved_model import layer_serialization\n",
    "from keras.utils import control_flow_util\n",
    "from keras.utils import generic_utils\n",
    "from keras.utils import tf_utils\n",
    "from keras.layers import RNN\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.util.tf_export import keras_export\n",
    "from tensorflow.tools.docs import doc_controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "214dbfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutRNNCellMixin(object):\n",
    "    \"\"\"Object that hold dropout related fields for RNN Cell.\n",
    "\n",
    "    This class is not a standalone RNN cell. It suppose to be used with a RNN cell\n",
    "    by multiple inheritance. Any cell that mix with class should have following\n",
    "    fields:\n",
    "    dropout: a float number within range [0, 1). The ratio that the input\n",
    "      tensor need to dropout.\n",
    "    recurrent_dropout: a float number within range [0, 1). The ratio that the\n",
    "      recurrent state weights need to dropout.\n",
    "    This object will create and cache created dropout masks, and reuse them for\n",
    "    the incoming data, so that the same mask is used for every batch input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self._create_non_trackable_mask_cache()\n",
    "        super(DropoutRNNCellMixin, self).__init__(*args, **kwargs)\n",
    "\n",
    "    @tf.__internal__.tracking.no_automatic_dependency_tracking\n",
    "    def _create_non_trackable_mask_cache(self):\n",
    "        \"\"\"Create the cache for dropout and recurrent dropout mask.\n",
    "\n",
    "        Note that the following two masks will be used in \"graph function\" mode,\n",
    "        e.g. these masks are symbolic tensors. In eager mode, the `eager_*_mask`\n",
    "        tensors will be generated differently than in the \"graph function\" case,\n",
    "        and they will be cached.\n",
    "\n",
    "        Also note that in graph mode, we still cache those masks only because the\n",
    "        RNN could be created with `unroll=True`. In that case, the `cell.call()`\n",
    "        function will be invoked multiple times, and we want to ensure same mask\n",
    "        is used every time.\n",
    "\n",
    "        Also the caches are created without tracking. Since they are not picklable\n",
    "        by python when deepcopy, we don't want `layer._obj_reference_counts_dict`\n",
    "        to track it by default.\n",
    "        \"\"\"\n",
    "        self._dropout_mask_cache = backend.ContextValueCache(\n",
    "            self._create_dropout_mask)\n",
    "        self._recurrent_dropout_mask_cache = backend.ContextValueCache(\n",
    "            self._create_recurrent_dropout_mask)\n",
    "\n",
    "    def reset_dropout_mask(self):\n",
    "        \"\"\"Reset the cached dropout masks if any.\n",
    "\n",
    "        This is important for the RNN layer to invoke this in it `call()` method so\n",
    "        that the cached mask is cleared before calling the `cell.call()`. The mask\n",
    "        should be cached across the timestep within the same batch, but shouldn't\n",
    "        be cached between batches. Otherwise it will introduce unreasonable bias\n",
    "        against certain index of data within the batch.\n",
    "        \"\"\"\n",
    "        self._dropout_mask_cache.clear()\n",
    "\n",
    "    def reset_recurrent_dropout_mask(self):\n",
    "        \"\"\"Reset the cached recurrent dropout masks if any.\n",
    "\n",
    "        This is important for the RNN layer to invoke this in it call() method so\n",
    "        that the cached mask is cleared before calling the cell.call(). The mask\n",
    "        should be cached across the timestep within the same batch, but shouldn't\n",
    "        be cached between batches. Otherwise it will introduce unreasonable bias\n",
    "        against certain index of data within the batch.\n",
    "        \"\"\"\n",
    "        self._recurrent_dropout_mask_cache.clear()\n",
    "\n",
    "    def _create_dropout_mask(self, inputs, training, count=1):\n",
    "        return _generate_dropout_mask(\n",
    "            tf.ones_like(inputs),\n",
    "            self.dropout,\n",
    "            training=training,\n",
    "            count=count)\n",
    "\n",
    "    def _create_recurrent_dropout_mask(self, inputs, training, count=1):\n",
    "        return _generate_dropout_mask(\n",
    "            tf.ones_like(inputs),\n",
    "            self.recurrent_dropout,\n",
    "            training=training,\n",
    "            count=count)\n",
    "\n",
    "    def get_dropout_mask_for_cell(self, inputs, training, count=1):\n",
    "        \"\"\"Get the dropout mask for RNN cell's input.\n",
    "\n",
    "        It will create mask based on context if there isn't any existing cached\n",
    "        mask. If a new mask is generated, it will update the cache in the cell.\n",
    "\n",
    "        Args:\n",
    "          inputs: The input tensor whose shape will be used to generate dropout\n",
    "            mask.\n",
    "          training: Boolean tensor, whether its in training mode, dropout will be\n",
    "            ignored in non-training mode.\n",
    "          count: Int, how many dropout mask will be generated. It is useful for cell\n",
    "            that has internal weights fused together.\n",
    "        Returns:\n",
    "          List of mask tensor, generated or cached mask based on context.\n",
    "        \"\"\"\n",
    "        if self.dropout == 0:\n",
    "            return None\n",
    "        init_kwargs = dict(inputs=inputs, training=training, count=count)\n",
    "        return self._dropout_mask_cache.setdefault(kwargs=init_kwargs)\n",
    "\n",
    "    def get_recurrent_dropout_mask_for_cell(self, inputs, training, count=1):\n",
    "        \"\"\"Get the recurrent dropout mask for RNN cell.\n",
    "\n",
    "        It will create mask based on context if there isn't any existing cached\n",
    "        mask. If a new mask is generated, it will update the cache in the cell.\n",
    "\n",
    "        Args:\n",
    "          inputs: The input tensor whose shape will be used to generate dropout\n",
    "            mask.\n",
    "          training: Boolean tensor, whether its in training mode, dropout will be\n",
    "            ignored in non-training mode.\n",
    "          count: Int, how many dropout mask will be generated. It is useful for cell\n",
    "            that has internal weights fused together.\n",
    "        Returns:\n",
    "          List of mask tensor, generated or cached mask based on context.\n",
    "        \"\"\"\n",
    "        if self.recurrent_dropout == 0:\n",
    "            return None\n",
    "        init_kwargs = dict(inputs=inputs, training=training, count=count)\n",
    "        return self._recurrent_dropout_mask_cache.setdefault(kwargs=init_kwargs)\n",
    "\n",
    "    def __getstate__(self):\n",
    "        # Used for deepcopy. The caching can't be pickled by python, since it will\n",
    "        # contain tensor and graph.\n",
    "        state = super(DropoutRNNCellMixin, self).__getstate__()\n",
    "        state.pop('_dropout_mask_cache', None)\n",
    "        state.pop('_recurrent_dropout_mask_cache', None)\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        state['_dropout_mask_cache'] = backend.ContextValueCache(\n",
    "            self._create_dropout_mask)\n",
    "        state['_recurrent_dropout_mask_cache'] = backend.ContextValueCache(\n",
    "            self._create_recurrent_dropout_mask)\n",
    "        super(DropoutRNNCellMixin, self).__setstate__(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50c60ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Mode implementation\n",
    "\n",
    "class MultiModalLSTMCell(DropoutRNNCellMixin, Layer):\n",
    "    def __init__(self,\n",
    "               units,\n",
    "               activation='softmax',\n",
    "               recurrent_activation='hard_sigmoid',\n",
    "               use_bias=True,\n",
    "               kernel_initializer='glorot_uniform',\n",
    "               recurrent_initializer='orthogonal',\n",
    "               bias_initializer='zeros',\n",
    "               unit_forget_bias=True,\n",
    "               kernel_regularizer=None,\n",
    "               recurrent_regularizer=None,\n",
    "               bias_regularizer=None,\n",
    "               kernel_constraint=None,\n",
    "               recurrent_constraint=None,\n",
    "               bias_constraint=None,\n",
    "               dropout=0.,\n",
    "               recurrent_dropout=0.,\n",
    "               **kwargs):\n",
    "        if tf.compat.v1.executing_eagerly_outside_functions():\n",
    "            self._enable_caching_device = kwargs.pop('enable_caching_device', True)\n",
    "        else:\n",
    "            self._enable_caching_device = kwargs.pop('enable_caching_device', False)\n",
    "        super(MultiModalLSTMCell, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.recurrent_activation = activations.get(recurrent_activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.unit_forget_bias = unit_forget_bias\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "        self.dropout = min(1., max(0., dropout))\n",
    "        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
    "        implementation = kwargs.pop('implementation', 1)\n",
    "        if self.recurrent_dropout != 0 and implementation != 1:\n",
    "            logging.debug(RECURRENT_DROPOUT_WARNING_MSG)\n",
    "            self.implementation = 1\n",
    "        else:\n",
    "            self.implementation = implementation\n",
    "            \n",
    "        self.state_size = [self.units, self.units]\n",
    "        self.output_size = self.units\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        print(input_shape)\n",
    "        default_caching_device = _caching_device(self)\n",
    "        input_dim = input_shape[-1]\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_dim, self.units * 9),\n",
    "            name='kernel',\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "            caching_device=default_caching_device)\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            shape=(self.units, self.units * 9),\n",
    "            name='recurrent_kernel',\n",
    "            initializer=self.recurrent_initializer,\n",
    "            regularizer=self.recurrent_regularizer,\n",
    "            constraint=self.recurrent_constraint,\n",
    "            caching_device=default_caching_device)\n",
    "\n",
    "        if self.use_bias:\n",
    "            if self.unit_forget_bias:\n",
    "                def bias_initializer(_, *args, **kwargs):\n",
    "                    return backend.concatenate([\n",
    "                      self.bias_initializer((self.units,), *args, **kwargs),\n",
    "                      initializers.get('ones')((self.units,), *args, **kwargs),\n",
    "                      self.bias_initializer((self.units * 2,), *args, **kwargs),\n",
    "                    ])\n",
    "            else:\n",
    "                bias_initializer = self.bias_initializer\n",
    "            self.bias = self.add_weight(\n",
    "                shape=(self.units * 9,),\n",
    "                name='bias',\n",
    "                initializer=bias_initializer,\n",
    "                regularizer=self.bias_regularizer,\n",
    "                constraint=self.bias_constraint,\n",
    "                caching_device=default_caching_device)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.built = True\n",
    "            \n",
    "    def _compute_carry_and_output(self, x, h_tm1, m1_tm1, m2_tm1):\n",
    "        \"\"\"Computes carry and output using split kernels.\"\"\"\n",
    "        x_i1, x_i2, x_f1, x_f2, x_m1, x_m2, x_o = x\n",
    "        h_tm1_i1, h_tm1_i2, h_tm1_f1, h_tm1_f2, h_tm1_m1, h_tm1_m2, h_tm1_o = h_tm1\n",
    "        i1 = self.recurrent_activation(x_i1 + backend.dot(h_tm1_i1, self.recurrent_kernel[:, :self.units]))\n",
    "        i2 = self.recurrent_activation(x_i2 + backend.dot(h_tm1_i2, self.recurrent_kernel[:, self.units:self.units*2]))\n",
    "        f1 = self.recurrent_activation(x_f1 + backend.dot(h_tm1_f1, self.recurrent_kernel[:, self.units*2:self.units * 3]))\n",
    "        f2 = self.recurrent_activation(x_f2 + backend.dot(h_tm1_f2, self.recurrent_kernel[:, self.units*3:self.units * 4]))\n",
    "        m1 = f1 * m1_tm1 + i1 * self.activation(x_m1 + backend.dot(h_tm1_m1, self.recurrent_kernel[:, self.units * 4:self.units * 5]))\n",
    "        m2 = f2 * m2_tm1 + i2 * self.activation(x_m2 + backend.dot(h_tm1_m2, self.recurrent_kernel[:, self.units * 5:self.units * 6]))\n",
    "        w1 = m1 * self.recurrent_activation(self.recurrent_kernel[:, self.units * 6 : self.units * 7])\n",
    "        w2 = m2 * self.recurrent_activation(self.recurrent_kernel[:, self.units * 7 : self.units * 8]) \n",
    "        o = w1 + w2 + self.activation(x_o + backend.dot(h_tm1_o, self.recurrent_kernel[:, self.units * 8:]))\n",
    "        return m1, m2, o\n",
    "    \n",
    "    def _compute_carry_and_output_fused(self, z, m1_tm1, m2_tm1):\n",
    "        \"\"\"Computes carry and output using fused kernels.\"\"\"\n",
    "        z0, z1, z2, z3, z4, z5, z6, z7, z8 = z\n",
    "        i1 = self.recurrent_activation(z0)\n",
    "        i2 = self.recurrent_activation(z1)\n",
    "        f1 = self.recurrent_activation(z2)\n",
    "        f2 = self.recurrent_activation(z3)\n",
    "        m1 = f1 * m1_tm1 + i1 * self.activation(z4)\n",
    "        m2 = f2 * m2_tm1 + i2 * self.activation(z5)\n",
    "        w1 = m1 * self.recurrent_activation(z6)\n",
    "        w2 = m2 * self.recurrent_activation(z7)\n",
    "        o = w1 + w2 + self.activation(z8)\n",
    "        return m1, m2, o\n",
    "\n",
    "    def call(self, inputs, states, training=None):\n",
    "        h_tm1 = states[0]  # previous memory state\n",
    "        m2_tm1 = states[1]  # previous carry state\n",
    "        m1_tm1 = states[2]\n",
    "\n",
    "        dp_mask = self.get_dropout_mask_for_cell(inputs, training, count=9)\n",
    "        rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(\n",
    "        h_tm1, training, count=9)\n",
    "\n",
    "        if self.implementation == 1:\n",
    "            if 0 < self.dropout < 1.:\n",
    "                inputs_i1 = inputs * dp_mask[0]\n",
    "                inputs_i2 = inputs * dp_mask[1]\n",
    "                inputs_f1 = inputs * dp_mask[2]\n",
    "                inputs_f2 = inputs * dp_mask[3]\n",
    "                inputs_m1 = inputs * dp_mask[4]\n",
    "                inputs_m2 = inputs * dp_mask[5]\n",
    "                inputs_w1 = inputs * dp_mask[6]\n",
    "                inputs_w2 = inputs * dp_mask[7]\n",
    "                inputs_o = inputs * dp_mask[8]\n",
    "            else:\n",
    "                inputs_i1 = inputs\n",
    "                inputs_i2 = inputs\n",
    "                inputs_f1 = inputs\n",
    "                inputs_f2 = inputs\n",
    "                inputs_m1 = inputs\n",
    "                inputs_m2 = inputs\n",
    "                inputs_w1 = inputs\n",
    "                inputs_w2 = inputs\n",
    "                inputs_o = inputs\n",
    "            k_i1, k_i2, k_f1, k_f2, k_m1, k_m2, k_w1, k_w2, k_o = tf.split(self.kernel, num_or_size_splits=9, axis=1)\n",
    "            x_i1 = backend.dot(inputs_i1, k_i1)\n",
    "            x_i2 = backend.dot(inputs_i2, k_i2)\n",
    "            x_f1 = backend.dot(inputs_f1, k_f1)\n",
    "            x_f2 = backend.dot(inputs_f2, k_f2)\n",
    "            x_m1 = backend.dot(inputs_m1, k_m1)\n",
    "            x_m2 = backend.dot(inputs_m2, k_m2)\n",
    "            x_w1 = backend.dot(inputs_w1, k_w1)\n",
    "            x_w2 = backend.dot(inputs_w2, k_w2)\n",
    "            x_o = backend.dot(inputs_o, k_o)\n",
    "            if self.use_bias:\n",
    "                b_i1, b_i2, b_f1, b_f2, b_m1, b_m2, b_w1, b_w2, b_o = tf.split(self.bias, num_or_size_splits=9, axis=0)\n",
    "                x_i1 = backend.bias_add(x_i1, b_i1)\n",
    "                x_i2 = backend.bias_add(x_i2, b_i2)\n",
    "                x_f1 = backend.bias_add(x_f1, b_f1)\n",
    "                x_f2 = backend.bias_add(x_f2, b_f2)\n",
    "                x_m1 = backend.bias_add(x_m1, b_m1)\n",
    "                x_m2 = backend.bias_add(x_m2, b_m2)\n",
    "                x_w1 = backend.bias_add(x_w1, b_w1)\n",
    "                x_w2 = backend.bias_add(x_w2, b_w2)\n",
    "                x_o = backend.bias_add(x_o, b_o)\n",
    "\n",
    "            if 0 < self.recurrent_dropout < 1.:\n",
    "                h_tm1_i1 = h_tm1 * rec_dp_mask[0]\n",
    "                h_tm1_i2 = h_tm1 * rec_dp_mask[1]\n",
    "                h_tm1_f1 = h_tm1 * rec_dp_mask[2]\n",
    "                h_tm1_f2 = h_tm1 * rec_dp_mask[3]\n",
    "                h_tm1_m1 = h_tm1 * rec_dp_mask[4]\n",
    "                h_tm1_m2 = h_tm1 * rec_dp_mask[5]\n",
    "                h_tm1_w1 = h_tm1 * rec_dp_mask[6]\n",
    "                h_tm1_w2 = h_tm1 * rec_dp_mask[7]\n",
    "                h_tm1_o = h_tm1 * rec_dp_mask[8]\n",
    "            else:\n",
    "                h_tm1_i1 = h_tm1\n",
    "                h_tm1_i2 = h_tm1\n",
    "                h_tm1_f1 = h_tm1\n",
    "                h_tm1_f2 = h_tm1\n",
    "                h_tm1_m1 = h_tm1\n",
    "                h_tm1_m2 = h_tm1\n",
    "                h_tm1_w1 = h_tm1\n",
    "                h_tm1_w2 = h_tm1\n",
    "                h_tm1_o = h_tm1\n",
    "            x = (x_i1, x_i2, x_f1, x_f2, x_m1, x_m2, x_w1, x_w2, x_o)\n",
    "            h_tm1 = (h_tm1_i1, h_tm1_i2, h_tm1_f1, h_tm1_f2, h_tm1_m1, h_tm1_m2, h_tm1_w1, h_tm1_w2, h_tm1_o)\n",
    "            m1, m2, o = self._compute_carry_and_output(x, h_tm1, m1_tm1, m2_tm1)\n",
    "        else:\n",
    "            if 0. < self.dropout < 1.:\n",
    "                inputs = inputs * dp_mask[0]\n",
    "            z = backend.dot(inputs, self.kernel)\n",
    "            z += backend.dot(h_tm1, self.recurrent_kernel)\n",
    "            if self.use_bias:\n",
    "                z = backend.bias_add(z, self.bias)\n",
    "            \n",
    "            z = tf.split(z, num_or_size_splits=9, axis=1)\n",
    "            m1, m2, o = self._compute_carry_and_output_fused(z, c_tm1)\n",
    "        \n",
    "        h = o\n",
    "        \n",
    "        return h\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'units':\n",
    "                self.units,\n",
    "            'activation':\n",
    "                activations.serialize(self.activation),\n",
    "            'recurrent_activation':\n",
    "                activations.serialize(self.recurrent_activation),\n",
    "            'use_bias':\n",
    "                self.use_bias,\n",
    "            'kernel_initializer':\n",
    "                initializers.serialize(self.kernel_initializer),\n",
    "            'recurrent_initializer':\n",
    "                initializers.serialize(self.recurrent_initializer),\n",
    "            'bias_initializer':\n",
    "                initializers.serialize(self.bias_initializer),\n",
    "            'unit_forget_bias':\n",
    "                self.unit_forget_bias,\n",
    "            'kernel_regularizer':\n",
    "                regularizers.serialize(self.kernel_regularizer),\n",
    "            'recurrent_regularizer':\n",
    "                regularizers.serialize(self.recurrent_regularizer),\n",
    "            'bias_regularizer':\n",
    "                regularizers.serialize(self.bias_regularizer),\n",
    "            'kernel_constraint':\n",
    "                constraints.serialize(self.kernel_constraint),\n",
    "            'recurrent_constraint':\n",
    "                constraints.serialize(self.recurrent_constraint),\n",
    "            'bias_constraint':\n",
    "                constraints.serialize(self.bias_constraint),\n",
    "            'dropout':\n",
    "                self.dropout,\n",
    "            'recurrent_dropout':\n",
    "                self.recurrent_dropout,\n",
    "            'implementation':\n",
    "                self.implementation\n",
    "        }\n",
    "        config.update(_config_for_enable_caching_device(self))\n",
    "        base_config = super(MultiModalLSTMCell, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n",
    "        return list(_generate_zero_filled_state_for_cell(\n",
    "            self, inputs, batch_size, dtype))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7595e752",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalLSTM(RNN):\n",
    "    def __init__(self,\n",
    "                 units,\n",
    "                 activation='softmax',\n",
    "                 recurrent_activation='hard_sigmoid',\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 recurrent_initializer='orthogonal',\n",
    "                 bias_initializer='zeros',\n",
    "                 unit_forget_bias=True,\n",
    "                 kernel_regularizer=None,\n",
    "                 recurrent_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 recurrent_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 dropout=0.,\n",
    "                 recurrent_dropout=0.,\n",
    "                 return_sequences=False,\n",
    "                 return_state=False,\n",
    "                 go_backwards=False,\n",
    "                 stateful=False,\n",
    "                 unroll=False,\n",
    "                 **kwargs):\n",
    "        implementation = kwargs.pop('implementation', 1)\n",
    "        if implementation == 0:\n",
    "            logging.warning('`implementation=0` has been deprecated, '\n",
    "                            'and now defaults to `implementation=1`.'\n",
    "                            'Please update your layer call.')\n",
    "        if 'enable_caching_device' in kwargs:\n",
    "            cell_kwargs = {'enable_caching_device':\n",
    "                           kwargs.pop('enable_caching_device')}\n",
    "        else:\n",
    "            cell_kwargs = {}\n",
    "        cell = MultiModalLSTMCell(units,\n",
    "                        activation=activation,\n",
    "                        recurrent_activation=recurrent_activation,\n",
    "                        use_bias=use_bias,\n",
    "                        kernel_initializer=kernel_initializer,\n",
    "                        recurrent_initializer=recurrent_initializer,\n",
    "                        unit_forget_bias=unit_forget_bias,\n",
    "                        bias_initializer=bias_initializer,\n",
    "                        kernel_regularizer=kernel_regularizer,\n",
    "                        recurrent_regularizer=recurrent_regularizer,\n",
    "                        bias_regularizer=bias_regularizer,\n",
    "                        kernel_constraint=kernel_constraint,\n",
    "                        recurrent_constraint=recurrent_constraint,\n",
    "                        bias_constraint=bias_constraint,\n",
    "                        dropout=dropout,\n",
    "                        recurrent_dropout=recurrent_dropout,\n",
    "                        implementation=implementation,\n",
    "                        dtype=kwargs.get('dtype'),\n",
    "                        trainable=kwargs.get('trainable', True),\n",
    "                        **cell_kwargs)\n",
    "        super(MultiModalLSTM, self).__init__(\n",
    "            cell,\n",
    "            return_sequences=return_sequences,\n",
    "            return_state=return_state,\n",
    "            go_backwards=go_backwards,\n",
    "            stateful=stateful,\n",
    "            unroll=unroll,\n",
    "            **kwargs)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        \n",
    "    def call(self, inputs, mask=None, training=None, initial_state=None):\n",
    "        return super(MultiModalLSTM, self).call(inputs, mask=mask, training=training, initial_state=initial_state)\n",
    "    \n",
    "    @property\n",
    "    def units(self):\n",
    "        return self.cell.units\n",
    "\n",
    "    @property\n",
    "    def activation(self):\n",
    "        return self.cell.activation\n",
    "\n",
    "    @property\n",
    "    def recurrent_activation(self):\n",
    "        return self.cell.recurrent_activation\n",
    "\n",
    "    @property\n",
    "    def use_bias(self):\n",
    "        return self.cell.use_bias\n",
    "\n",
    "    @property\n",
    "    def kernel_initializer(self):\n",
    "        return self.cell.kernel_initializer\n",
    "\n",
    "    @property\n",
    "    def recurrent_initializer(self):\n",
    "        return self.cell.recurrent_initializer\n",
    "\n",
    "    @property\n",
    "    def bias_initializer(self):\n",
    "        return self.cell.bias_initializer\n",
    "\n",
    "    @property\n",
    "    def unit_forget_bias(self):\n",
    "        return self.cell.unit_forget_bias\n",
    "\n",
    "    @property\n",
    "    def kernel_regularizer(self):\n",
    "        return self.cell.kernel_regularizer\n",
    "\n",
    "    @property\n",
    "    def recurrent_regularizer(self):\n",
    "        return self.cell.recurrent_regularizer\n",
    "\n",
    "    @property\n",
    "    def bias_regularizer(self):\n",
    "        return self.cell.bias_regularizer\n",
    "\n",
    "    @property\n",
    "    def kernel_constraint(self):\n",
    "        return self.cell.kernel_constraint\n",
    "\n",
    "    @property\n",
    "    def recurrent_constraint(self):\n",
    "        return self.cell.recurrent_constraint\n",
    "\n",
    "    @property\n",
    "    def bias_constraint(self):\n",
    "        return self.cell.bias_constraint\n",
    "\n",
    "    @property\n",
    "    def dropout(self):\n",
    "        return self.cell.dropout\n",
    "\n",
    "    @property\n",
    "    def recurrent_dropout(self):\n",
    "        return self.cell.recurrent_dropout\n",
    "\n",
    "    @property\n",
    "    def implementation(self):\n",
    "        return self.cell.implementation\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'units': \n",
    "                self.units,\n",
    "            'activation':\n",
    "                activations.serialize(self.activation),\n",
    "            'recurrent_activation':\n",
    "                activations.serialize(self.recurrent_activation),\n",
    "            'use_bias':\n",
    "                self.use_bias,\n",
    "            'kernel_initializer':\n",
    "                initializers.serialize(self.kernel_initializer),\n",
    "            'recurrent_initializer':\n",
    "                initializers.serialize(self.recurrent_initializer),\n",
    "            'bias_initializer':\n",
    "                initializers.serialize(self.bias_initializer),\n",
    "            'unit_forget_bias':\n",
    "                self.unit_forget_bias,\n",
    "            'kernel_regularizer':\n",
    "                regularizers.serialize(self.kernel_regularizer),\n",
    "            'recurrent_regularizer':\n",
    "                regularizers.serialize(self.recurrent_regularizer),\n",
    "            'bias_regularizer':\n",
    "                regularizers.serialize(self.bias_regularizer),\n",
    "            'activity_regularizer':\n",
    "                regularizers.serialize(self.activity_regularizer),\n",
    "            'kernel_constraint':\n",
    "                constraints.serialize(self.kernel_constraint),\n",
    "            'recurrent_constraint':\n",
    "                constraints.serialize(self.recurrent_constraint),\n",
    "            'bias_constraint':\n",
    "                constraints.serialize(self.bias_constraint),\n",
    "            'dropout':\n",
    "                self.dropout,\n",
    "            'recurrent_dropout':\n",
    "                self.recurrent_dropout,\n",
    "            'implementation':\n",
    "                self.implementation\n",
    "        }\n",
    "        config.update(_config_for_enable_caching_device(self.cell))\n",
    "        base_config = super(MultiModalLSTM, self).get_config()\n",
    "        del base_config['cell']\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        if 'implementation' in config and config['implementation'] == 0:\n",
    "            config['implementation'] = 1\n",
    "        return cls(**config)\n",
    "    \n",
    "def _generate_dropout_mask(ones, rate, training=None, count=1):\n",
    "    def dropped_inputs():\n",
    "        return backend.dropout(ones, rate)\n",
    "\n",
    "    if count > 1:\n",
    "        return [\n",
    "            backend.in_train_phase(dropped_inputs, ones, training=training)\n",
    "            for _ in range(count)\n",
    "        ]\n",
    "    \n",
    "    return backend.in_train_phase(dropped_inputs, ones, training=training)\n",
    "\n",
    "\n",
    "def _standardize_args(inputs, initial_state, constants, num_constants):\n",
    "    \"\"\"Standardizes `__call__` to a single list of tensor inputs.\n",
    "\n",
    "    When running a model loaded from a file, the input tensors\n",
    "    `initial_state` and `constants` can be passed to `RNN.__call__()` as part\n",
    "    of `inputs` instead of by the dedicated keyword arguments. This method\n",
    "    makes sure the arguments are separated and that `initial_state` and\n",
    "    `constants` are lists of tensors (or None).\n",
    "\n",
    "    Args:\n",
    "    inputs: Tensor or list/tuple of tensors. which may include constants\n",
    "      and initial states. In that case `num_constant` must be specified.\n",
    "    initial_state: Tensor or list of tensors or None, initial states.\n",
    "    constants: Tensor or list of tensors or None, constant tensors.\n",
    "    num_constants: Expected number of constants (if constants are passed as\n",
    "      part of the `inputs` list.\n",
    "\n",
    "    Returns:\n",
    "    inputs: Single tensor or tuple of tensors.\n",
    "    initial_state: List of tensors or None.\n",
    "    constants: List of tensors or None.\n",
    "    \"\"\"\n",
    "    if isinstance(inputs, list):\n",
    "        # There are several situations here:\n",
    "        # In the graph mode, __call__ will be only called once. The initial_state\n",
    "        # and constants could be in inputs (from file loading).\n",
    "        # In the eager mode, __call__ will be called twice, once during\n",
    "        # rnn_layer(inputs=input_t, constants=c_t, ...), and second time will be\n",
    "        # model.fit/train_on_batch/predict with real np data. In the second case,\n",
    "        # the inputs will contain initial_state and constants as eager tensor.\n",
    "        #\n",
    "        # For either case, the real input is the first item in the list, which\n",
    "        # could be a nested structure itself. Then followed by initial_states, which\n",
    "        # could be a list of items, or list of list if the initial_state is complex\n",
    "        # structure, and finally followed by constants which is a flat list.\n",
    "        assert initial_state is None and constants is None\n",
    "        if num_constants:\n",
    "            constants = inputs[-num_constants:]\n",
    "            inputs = inputs[:-num_constants]\n",
    "        if len(inputs) > 1:\n",
    "            initial_state = inputs[1:]\n",
    "            inputs = inputs[:1]\n",
    "            \n",
    "        if len(inputs) > 1:\n",
    "            inputs = tuple(inputs)\n",
    "        else:\n",
    "            inputs = inputs[0]\n",
    "\n",
    "    def to_list_or_none(x):\n",
    "        if x is None or isinstance(x, list):\n",
    "            return x\n",
    "        if isinstance(x, tuple):\n",
    "            return list(x)\n",
    "        return [x]\n",
    "\n",
    "    initial_state = to_list_or_none(initial_state)\n",
    "    constants = to_list_or_none(constants)\n",
    "    \n",
    "    return inputs, initial_state, constants\n",
    "\n",
    "\n",
    "def _is_multiple_state(state_size):\n",
    "    \"\"\"Check whether the state_size contains multiple states.\"\"\"\n",
    "    return (hasattr(state_size, '__len__') and\n",
    "            not isinstance(state_size, tf.TensorShape))\n",
    "\n",
    "\n",
    "def _generate_zero_filled_state_for_cell(cell, inputs, batch_size, dtype):\n",
    "    if inputs is not None:\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        dtype = inputs.dtype\n",
    "    return _generate_zero_filled_state(batch_size, cell.state_size, dtype)\n",
    "\n",
    "\n",
    "def _generate_zero_filled_state(batch_size_tensor, state_size, dtype):\n",
    "    \"\"\"Generate a zero filled tensor with shape [batch_size, state_size].\"\"\"\n",
    "    if batch_size_tensor is None or dtype is None:\n",
    "        raise ValueError(\n",
    "            'batch_size and dtype cannot be None while constructing initial state: '\n",
    "            'batch_size={}, dtype={}'.format(batch_size_tensor, dtype))\n",
    "\n",
    "    def create_zeros(unnested_state_size):\n",
    "        flat_dims = tf.TensorShape(unnested_state_size).as_list()\n",
    "        init_state_size = [batch_size_tensor] + flat_dims\n",
    "        return tf.zeros(init_state_size, dtype=dtype)\n",
    "\n",
    "    if tf.nest.is_nested(state_size):\n",
    "        return tf.nest.map_structure(create_zeros, state_size)\n",
    "    else:\n",
    "        return create_zeros(state_size)\n",
    "\n",
    "\n",
    "def _caching_device(rnn_cell):\n",
    "    \"\"\"Returns the caching device for the RNN variable.\n",
    "\n",
    "    This is useful for distributed training, when variable is not located as same\n",
    "    device as the training worker. By enabling the device cache, this allows\n",
    "    worker to read the variable once and cache locally, rather than read it every\n",
    "    time step from remote when it is needed.\n",
    "\n",
    "    Note that this is assuming the variable that cell needs for each time step is\n",
    "    having the same value in the forward path, and only gets updated in the\n",
    "    backprop. It is true for all the default cells (SimpleRNN, GRU, LSTM). If the\n",
    "    cell body relies on any variable that gets updated every time step, then\n",
    "    caching device will cause it to read the stall value.\n",
    "\n",
    "    Args:\n",
    "    rnn_cell: the rnn cell instance.\n",
    "    \"\"\"\n",
    "    if tf.executing_eagerly():\n",
    "        # caching_device is not supported in eager mode.\n",
    "        return None\n",
    "    if not getattr(rnn_cell, '_enable_caching_device', False):\n",
    "        return None\n",
    "    # Don't set a caching device when running in a loop, since it is possible that\n",
    "    # train steps could be wrapped in a tf.while_loop. In that scenario caching\n",
    "    # prevents forward computations in loop iterations from re-reading the\n",
    "    # updated weights.\n",
    "    if control_flow_util.IsInWhileLoop(tf.compat.v1.get_default_graph()):\n",
    "        logging.warning(\n",
    "            'Variable read device caching has been disabled because the '\n",
    "            'RNN is in tf.while_loop loop context, which will cause '\n",
    "            'reading stalled value in forward path. This could slow down '\n",
    "            'the training due to duplicated variable reads. Please '\n",
    "            'consider updating your code to remove tf.while_loop if possible.')\n",
    "        return None\n",
    "    if (rnn_cell._dtype_policy.compute_dtype !=\n",
    "        rnn_cell._dtype_policy.variable_dtype):\n",
    "        logging.warning(\n",
    "            'Variable read device caching has been disabled since it '\n",
    "            'doesn\\'t work with the mixed precision API. This is '\n",
    "            'likely to cause a slowdown for RNN training due to '\n",
    "            'duplicated read of variable for each timestep, which '\n",
    "            'will be significant in a multi remote worker setting. '\n",
    "            'Please consider disabling mixed precision API if '\n",
    "            'the performance has been affected.')\n",
    "        return None\n",
    "    # Cache the value on the device that access the variable.\n",
    "    return lambda op: op.device\n",
    "\n",
    "\n",
    "def _config_for_enable_caching_device(rnn_cell):\n",
    "    \"\"\"Return the dict config for RNN cell wrt to enable_caching_device field.\n",
    "\n",
    "    Since enable_caching_device is a internal implementation detail for speed up\n",
    "    the RNN variable read when running on the multi remote worker setting, we\n",
    "    don't want this config to be serialized constantly in the JSON. We will only\n",
    "    serialize this field when a none default value is used to create the cell.\n",
    "    Args:\n",
    "    rnn_cell: the RNN cell for serialize.\n",
    "\n",
    "    Returns:\n",
    "    A dict which contains the JSON config for enable_caching_device value or\n",
    "    empty dict if the enable_caching_device value is same as the default value.\n",
    "    \"\"\"\n",
    "    default_enable_caching_device = tf.compat.v1.executing_eagerly_outside_functions()\n",
    "    if rnn_cell._enable_caching_device != default_enable_caching_device:\n",
    "        return {'enable_caching_device': rnn_cell._enable_caching_device}\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d95bb30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6010ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"the-circor-digiscope-phonocardiogram-dataset-1.0.3/training_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "663ed229",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id = list(df[\"Patient ID\"])\n",
    "recording_loc = list(df[\"Recording locations:\"])\n",
    "murmur = list(df[\"Murmur\"])\n",
    "murmur_loc = list(df[\"Murmur locations\"])\n",
    "systolic_murmur_timing = list(df[\"Systolic murmur timing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcea2f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_loc = [x.split(\"+\") for x in recording_loc]\n",
    "\n",
    "for i in range(len(murmur_loc)):\n",
    "    if murmur_loc[i] is np.nan:\n",
    "        murmur_loc[i] = []\n",
    "    else:\n",
    "        murmur_loc[i] = murmur_loc[i].split(\"+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22376f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on patient ID: 85119\n",
      "Error on patient ID: 85119\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(len(patient_id)):\n",
    "    if murmur[i] == \"Absent\":\n",
    "        for recloc in recording_loc[i]:\n",
    "            full_recording = np.loadtxt(\"SSE/\" + str(patient_id[i]) + \"_\" + recloc + \"_features.csv\", delimiter=',')\n",
    "            initial = 0\n",
    "            while (initial+400) <= len(full_recording):\n",
    "                X.append(full_recording[initial:initial+400])\n",
    "                y.append([1, 0, 0, 0, 0])\n",
    "                initial += 200\n",
    "    elif murmur[i] == \"Present\":\n",
    "        for recloc in recording_loc[i]:\n",
    "            if recloc in murmur_loc[i]:\n",
    "                full_recording = np.loadtxt(\"SSE/\" + str(patient_id[i]) + \"_\" + recloc + \"_features.csv\", delimiter=',')\n",
    "                initial = 0\n",
    "                if systolic_murmur_timing[i] == \"Holosystolic\":\n",
    "                    while (initial+400) <= len(full_recording):\n",
    "                        X.append(full_recording[initial:initial+400])\n",
    "                        y.append([0, 1, 0, 0, 0])\n",
    "                        initial += 200\n",
    "                elif systolic_murmur_timing[i] == \"Early-systolic\":\n",
    "                    while (initial+400) <= len(full_recording):\n",
    "                        X.append(full_recording[initial:initial+400])\n",
    "                        y.append([0, 0, 1, 0, 0])\n",
    "                        initial += 200\n",
    "                elif systolic_murmur_timing[i] == \"Mid-systolic\":\n",
    "                    while (initial+400) <= len(full_recording):\n",
    "                        X.append(full_recording[initial:initial+400])\n",
    "                        y.append([0, 0, 0, 1, 0])\n",
    "                        initial += 200\n",
    "                elif systolic_murmur_timing[i] == \"Late-systolic\":\n",
    "                    while (initial+400) <= len(full_recording):\n",
    "                        X.append(full_recording[initial:initial+400])\n",
    "                        y.append([0, 0, 0, 0, 1])\n",
    "                        initial += 200\n",
    "                else:\n",
    "                    print(\"Error on patient ID:\", patient_id[i])\n",
    "            else:\n",
    "                full_recording = np.loadtxt(\"SSE/\" + str(patient_id[i]) + \"_\" + recloc + \"_features.csv\", delimiter=',')\n",
    "                initial = 0\n",
    "                while (initial+400) <= len(full_recording):\n",
    "                    X.append(full_recording[initial:initial+400])\n",
    "                    y.append([1, 0, 0, 0, 0])\n",
    "                    initial += 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d95a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 400\n",
    "feature_length = 18\n",
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c7f230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "X_train, X_test, y_train, y_test = tts(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c098ba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = []\n",
    "y_train_final = []\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    if y_train[i] == [1, 0, 0, 0, 0]:\n",
    "        #randnum = random.uniform(0, 1)\n",
    "        randnum = random.random()\n",
    "        if randnum >= 0.66:\n",
    "            X_train_final.append(X_train[i])\n",
    "            y_train_final.append(y_train[i])\n",
    "    else:\n",
    "        X_train_final.append(X_train[i])\n",
    "        y_train_final.append(y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e920503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22341 22341\n",
      "10121 10121\n",
      "7447 7447\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(y_train))\n",
    "print(len(X_train_final), len(y_train_final))\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90e0a6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_train_final = np.array(X_train_final)\n",
    "y_train_final = np.array(y_train_final)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c3c3948",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The initial value's shape ((1024,)) is not compatible with the explicitly supplied `shape` argument ((2304,)).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-208e051c4271>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMultiModalLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m18\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMultiModalLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    200\u001b[0m           \u001b[1;31m# and create the node connecting the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m           \u001b[1;31m# to the input layer we just created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m           \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m           \u001b[0mset_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 659\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m     \u001b[1;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[1;32m--> 977\u001b[1;33m                                                 input_list)\n\u001b[0m\u001b[0;32m    978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m     \u001b[1;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1113\u001b[0m       \u001b[1;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m       outputs = self._keras_tensor_symbolic_call(\n\u001b[1;32m-> 1115\u001b[1;33m           inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[0;32m   1116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1117\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[1;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[0;32m    846\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[1;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[0;32m    884\u001b[0m           \u001b[1;31m# overridden).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    885\u001b[0m           \u001b[1;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 886\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    887\u001b[0m           \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2657\u001b[0m         \u001b[1;31m# operations.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2658\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2659\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint:disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2660\u001b[0m       \u001b[1;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2661\u001b[0m       \u001b[1;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    575\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLayer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 577\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_input_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    578\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-648931847cb0>\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[0mregularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_regularizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_constraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                 caching_device=default_caching_device)\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[0;32m    661\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 663\u001b[1;33m         caching_device=caching_device)\n\u001b[0m\u001b[0;32m    664\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m       \u001b[1;31m# TODO(fchollet): in the future, this should be handled at the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[1;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[0;32m    816\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 818\u001b[1;33m         **kwargs_for_getter)\n\u001b[0m\u001b[0;32m    819\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m     \u001b[1;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\base_layer_utils.py\u001b[0m in \u001b[0;36mmake_variable\u001b[1;34m(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m       \u001b[0maggregation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m       shape=variable_shape if variable_shape else None)\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mVariableV1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m_variable_v1_call\u001b[1;34m(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m         shape=shape)\n\u001b[0m\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m   def _variable_v2_call(cls,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m                         shape=None):\n\u001b[0;32m    204\u001b[0m     \u001b[1;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m     \u001b[0mprevious_getter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdefault_variable_creator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator\u001b[1;34m(next_creator, **kwargs)\u001b[0m\n\u001b[0;32m   2624\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2625\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2626\u001b[1;33m         shape=shape)\n\u001b[0m\u001b[0;32m   2627\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2628\u001b[0m     return variables.RefVariable(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    268\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 270\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[0;32m   1611\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1612\u001b[0m           \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1613\u001b[1;33m           distribute_strategy=distribute_strategy)\n\u001b[0m\u001b[0;32m   1614\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1615\u001b[0m   def _init_from_args(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[1;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[0;32m   1751\u001b[0m                   \u001b[1;34m\"The initial value's shape (%s) is not compatible with \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1752\u001b[0m                   \u001b[1;34m\"the explicitly supplied `shape` argument (%s).\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1753\u001b[1;33m                   (initial_value.shape, shape))\n\u001b[0m\u001b[0;32m   1754\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1755\u001b[0m             \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The initial value's shape ((1024,)) is not compatible with the explicitly supplied `shape` argument ((2304,))."
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(MultiModalLSTM(256, input_shape = (400, 18), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MultiModalLSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MultiModalLSTM(64, return_sequences=False))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd5f12f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
